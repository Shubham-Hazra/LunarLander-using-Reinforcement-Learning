{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b438f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp lunarlander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ed269fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Import the relevant libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from pyvirtualdisplay import Display\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from PIL import Image\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "336e4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for TensorFlow\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a6948c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8d3fb43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "env.action_space.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0e2a91a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cae12d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment and get the initial state.\n",
    "initial_state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "101ed9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: (array([0.007, 1.419, 0.689, 0.377, -0.008, -0.156, 0.000, 0.000],\n",
      "      dtype=float32), {})\n",
      "Action: 0\n",
      "Next State: [0.014 1.427 0.688 0.352 -0.016 -0.154 0.000 0.000]\n",
      "Reward Received: -0.28294864096122296\n",
      "Episode Terminated: False\n",
      "Info: {}\n"
     ]
    }
   ],
   "source": [
    "# Select an action\n",
    "action = 0\n",
    "\n",
    "# Run a single time step of the environment's dynamics with the given action.\n",
    "next_state, reward, terminated,truncated, info = env.step(action)\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.3f}'.format}):\n",
    "    print(\"Initial State:\", initial_state)\n",
    "    print(\"Action:\", action)\n",
    "    print(\"Next State:\", next_state)\n",
    "    print(\"Reward Received:\", reward)\n",
    "    print(\"Episode Terminated:\", truncated)\n",
    "    print(\"Info:\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e0955f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dense(num_actions,activation='linear')\n",
    "]) \n",
    "\n",
    "target_q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dense(num_actions,activation='linear')\n",
    "]) \n",
    "\n",
    "optimizer = Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eb80522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f56ed1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \n",
    "    states, actions,rewards,next_states,done_vals = experiences\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "    y_targets = rewards + (gamma * max_qsa * (1 - done_vals))\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "    loss = MSE(y_targets, q_values)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "743b6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def agent_learn(experiences, gamma):    \n",
    "    # Calculate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the weights.\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    # Update the weights of the q_network.\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    # update the weights of target q_network\n",
    "    for target_weights, q_net_weights in zip(target_q_network.weights, q_network.weights):\n",
    "        target_weights.assign(TAU * q_net_weights + (1.0 - TAU) * target_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "29ceac80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [ 0.00552673  1.4122307   0.5597782   0.05824031 -0.00639727 -0.12679817\n",
      "  0.          0.        ]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"sequential_16\" \"                 f\"(type Sequential).\n\nInput 0 of layer \"dense_48\" is incompatible with the layer: expected axis -1 of input shape to have value 8, but received input with shape (1, 1)\n\nCall arguments received by layer \"sequential_16\" \"                 f\"(type Sequential):\n  • inputs=tf.Tensor(shape=(1, 1), dtype=float32)\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [109]\u001b[0m, in \u001b[0;36m<cell line: 53>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m state_qn \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(state_qn, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     65\u001b[0m state_qn \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(state_qn, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;66;03m# state needs to be the right shape for the q_network\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[43mq_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_qn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m action \u001b[38;5;241m=\u001b[39m get_action(q_values, epsilon)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Take action A and receive reward R and the next state S'\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py:277\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    272\u001b[0m             value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape_as_list[\u001b[38;5;28mint\u001b[39m(axis)] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    274\u001b[0m             value,\n\u001b[0;32m    275\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m         }:\n\u001b[1;32m--> 277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    278\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    279\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_shape(x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m             )\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"sequential_16\" \"                 f\"(type Sequential).\n\nInput 0 of layer \"dense_48\" is incompatible with the layer: expected axis -1 of input shape to have value 8, but received input with shape (1, 1)\n\nCall arguments received by layer \"sequential_16\" \"                 f\"(type Sequential):\n  • inputs=tf.Tensor(shape=(1, 1), dtype=float32)\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "SEED = 0              # seed for pseudo-random number generator\n",
    "MINIBATCH_SIZE = 64   # mini-batch size\n",
    "TAU = 1e-3            # soft update parameter\n",
    "E_DECAY = 0.995       # ε decay rate for ε-greedy policy\n",
    "E_MIN = 0.01          # minimum ε value for ε-greedy policy\n",
    "\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "def get_experiences(memory_buffer):\n",
    "    experiences = random.sample(memory_buffer, k=MINIBATCH_SIZE)\n",
    "    states = tf.convert_to_tensor(np.array([e.state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(np.array([e.action for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    rewards = tf.convert_to_tensor(np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    next_states = tf.convert_to_tensor(np.array([e.next_state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    done_vals = tf.convert_to_tensor(np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
    "                                     dtype=tf.float32)\n",
    "    return (states, actions, rewards, next_states, done_vals)\n",
    "\n",
    "\n",
    "def check_update_conditions(t, num_steps_upd, memory_buffer):\n",
    "    if (t + 1) % num_steps_upd == 0 and len(memory_buffer) > MINIBATCH_SIZE:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def get_new_eps(epsilon):\n",
    "    return max(E_MIN, E_DECAY*epsilon)\n",
    "\n",
    "def get_action(q_values, epsilon=0):\n",
    "    if random.random() > epsilon:\n",
    "        return np.argmax(q_values.numpy()[0])\n",
    "    else:\n",
    "        return random.choice(np.arange(4))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "num_episodes = 2000\n",
    "max_num_timesteps = 1000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100    # number of total points to use for averaging\n",
    "epsilon = 1.0     # initial ε value for ε-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# Set the target network weights equal to the Q-Network weights\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # Reset the environment to the initial state and get the initial state\n",
    "    state = env.reset()[0]\n",
    "    total_points = 0\n",
    "    \n",
    "    for t in range(max_num_timesteps):\n",
    "        \n",
    "        # From the current state S choose an action A using an ε-greedy policy\n",
    "        print(f\"State: {state}\")\n",
    "        state_qn = state[0]\n",
    "        state_qn = np.expand_dims(state_qn, axis=0)  # state needs to be the right shape for the q_network\n",
    "        q_values = q_network(state_qn)\n",
    "        action = get_action(q_values, epsilon)\n",
    "        \n",
    "        # Take action A and receive reward R and the next state S'\n",
    "        next_state, reward,terminated,truncated,_ = env.step(action)\n",
    "        print(next_state)\n",
    "        done = (terminated or truncated)\n",
    "        # Store experience tuple (S,A,R,S') in the memory buffer.\n",
    "        # We store the done variable as well for convenience.\n",
    "        memory_buffer.append(experience(state, action, reward, next_state,done))\n",
    "        \n",
    "        # Only update the network every NUM_STEPS_FOR_UPDATE time steps.\n",
    "        update = check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "        \n",
    "        if update:\n",
    "            # Sample random mini-batch of experience tuples (S,A,R,S') from D\n",
    "            experiences = utils.get_experiences(memory_buffer)\n",
    "            \n",
    "            # Set the y targets, perform a gradient descent step,\n",
    "            # and update the network weights.\n",
    "            agent_learn(experiences, GAMMA)\n",
    "        \n",
    "        state = next_state.copy()\n",
    "        total_points += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "    \n",
    "    # Update the ε value\n",
    "    epsilon = get_new_eps(epsilon)\n",
    "\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "\n",
    "    # We will consider that the environment is solved if we get an\n",
    "    # average of 200 points in the last 100 episodes.\n",
    "    if av_latest_points >= 200.0:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_network.save('lunar_lander_model.h5')\n",
    "        break\n",
    "        \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7455ebb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [76]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m     ax\u001b[38;5;241m.\u001b[39mtick_params(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, colors\u001b[38;5;241m=\u001b[39mtext_color)\n\u001b[0;32m     34\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 36\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241m.\u001b[39mplot_history(total_point_history)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the point history\n",
    "\n",
    "def plot_history(reward_history, rolling_window=20, lower_limit=None,\n",
    "                 upper_limit=None, plot_rw=True, plot_rm=True):\n",
    "    \n",
    "    if lower_limit is None or upper_limit is None:\n",
    "        rh = reward_history\n",
    "        xs = [x for x in range(len(reward_history))]\n",
    "    else:\n",
    "        rh = reward_history[lower_limit:upper_limit]\n",
    "        xs = [x for x in range(lower_limit,upper_limit)]\n",
    "    \n",
    "    df = pd.DataFrame(rh)\n",
    "    rollingMean = df.rolling(rolling_window).mean()\n",
    "\n",
    "    plt.figure(figsize=(10,7), facecolor='white')\n",
    "    \n",
    "    if plot_rw:\n",
    "        plt.plot(xs, rh, linewidth=1, color='cyan')\n",
    "    if plot_rm:\n",
    "        plt.plot(xs, rollingMean, linewidth=2, color='magenta')\n",
    "    text_color = 'black'\n",
    "        \n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('black')\n",
    "    plt.grid()\n",
    "#     plt.title(\"Total Point History\", color=text_color, fontsize=40)\n",
    "    plt.xlabel('Episode', color=text_color, fontsize=30)\n",
    "    plt.ylabel('Total Points', color=text_color, fontsize=30)\n",
    "    yNumFmt = mticker.StrMethodFormatter('{x:,}')\n",
    "    ax.yaxis.set_major_formatter(yNumFmt)\n",
    "    ax.tick_params(axis='x', colors=text_color)\n",
    "    ax.tick_params(axis='y', colors=text_color)\n",
    "    plt.show()\n",
    "\n",
    "utils.plot_history(total_point_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev\n",
    "nbdev.export.nb_export('lunarlander.ipynb','lunarlander.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5285492d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
