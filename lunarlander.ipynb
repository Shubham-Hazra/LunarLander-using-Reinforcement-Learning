{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b438f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp lunarlander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed269fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Import the relevant libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import deque, namedtuple\n",
    "from pyvirtualdisplay import Display\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from PIL import Image\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6948c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "# Initialize all the parameters\n",
    "MEMORY_SIZE = 100000    \n",
    "GAMMA = 0.995            \n",
    "LEARNING_RATE = 1e-3               \n",
    "NUM_STEPS_FOR_UPDATE = 4 \n",
    "SEED = 0              \n",
    "MINIBATCH_SIZE = 64   \n",
    "TAU = 1e-3           \n",
    "E_DECAY = 0.995      \n",
    "E_MIN = 0.01       \n",
    "\n",
    "# Using Namedtuple to store the experience\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "# Set the seed\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d3fb43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Initialize the gym environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0955f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Define the models and optimizer\n",
    "q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dense(num_actions,activation='linear')\n",
    "]) \n",
    "\n",
    "target_q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dense(num_actions,activation='linear')\n",
    "]) \n",
    "\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f56ed1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Define the loss function\n",
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \n",
    "    states, actions,rewards,next_states,done_vals = experiences\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "    y_targets = rewards + (gamma * max_qsa * (1 - done_vals))\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "    loss = MSE(y_targets, q_values)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "743b6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Define the update step\n",
    "@tf.function\n",
    "def agent_learn(experiences, gamma):    \n",
    "    # Calculate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the weights.\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    # Update the weights of the q_network.\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    # update the weights of target q_network\n",
    "    for target_weights, q_net_weights in zip(target_q_network.weights, q_network.weights):\n",
    "        target_weights.assign(TAU * q_net_weights + (1.0 - TAU) * target_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "181714e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Function to retrieve the experiences\n",
    "def get_experiences(memory_buffer):\n",
    "    experiences = random.sample(memory_buffer, k=MINIBATCH_SIZE)\n",
    "    states = tf.convert_to_tensor(np.array([e.state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(np.array([e.action for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    rewards = tf.convert_to_tensor(np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    next_states = tf.convert_to_tensor(np.array([e.next_state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    done_vals = tf.convert_to_tensor(np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
    "                                     dtype=tf.float32)\n",
    "    return (states, actions, rewards, next_states, done_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1938bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "# Funtion to check for the update condition\n",
    "def check_update_conditions(t, num_steps_upd, memory_buffer):\n",
    "    if (t + 1) % num_steps_upd == 0 and len(memory_buffer) > MINIBATCH_SIZE:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc7e06c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "# Function to update and get the new epsilon\n",
    "def get_new_eps(epsilon):\n",
    "    return max(E_MIN, E_DECAY*epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ccf00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Function to get the next action\n",
    "def get_action(q_values, epsilon=0):\n",
    "    if random.random() > epsilon:\n",
    "        return np.argmax(q_values.numpy()[0])\n",
    "    else:\n",
    "        return random.choice(np.arange(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d9266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# The training loop\n",
    "start = time.time()\n",
    "\n",
    "num_episodes = 2000\n",
    "max_num_timesteps = 1000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100    # number of total points to use for averaging\n",
    "epsilon = 1.0     # initial epsilon value for epsilon-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# Set the target network weights equal to the Q-Network weights\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # Reset the environment to the initial state and get the initial state\n",
    "    state = env.reset()\n",
    "    total_points = 0\n",
    "\n",
    "    for t in range(max_num_timesteps):\n",
    "\n",
    "        # From the current state S choose an action A using an epsilon-greedy policy\n",
    "        state_qn = np.expand_dims(state[0], axis=0)  # state needs to be the right shape for the q_network\n",
    "\n",
    "        q_values = q_network(tf.convert_to_tensor(state_qn,dtype=tf.float32))\n",
    "        action = get_action(q_values, epsilon)\n",
    "        \n",
    "        # Take action A and receive reward R and the next state S'\n",
    "        next_state, reward, terminated, truncated,_ = env.step(action)\n",
    "        done = (terminated or truncated)\n",
    "        \n",
    "        # Store experience tuple (S,A,R,S') in the memory buffer.\n",
    "        # We store the done variable as well for convenience.\n",
    "        memory_buffer.append(experience(state[0], action, reward, next_state, done))\n",
    "        \n",
    "        # Only update the network every NUM_STEPS_FOR_UPDATE time steps.\n",
    "        update = check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "        \n",
    "        if update:\n",
    "            # Sample random mini-batch of experience tuples (S,A,R,S') from D\n",
    "            experiences = get_experiences(memory_buffer)\n",
    "            \n",
    "            # Set the y targets, perform a gradient descent step,\n",
    "            # and update the network weights.\n",
    "            agent_learn(experiences, GAMMA)\n",
    "        \n",
    "        state = (next_state.copy(),_)\n",
    "        total_points += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "    \n",
    "    # Update the epsilon value\n",
    "    epsilon = get_new_eps(epsilon)\n",
    "\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "\n",
    "    # We will consider that the environment is solved if we get an\n",
    "    # average of 200 points in the last 100 episodes.\n",
    "    if av_latest_points >= 200.0:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_network.save('lunar_lander_model.h5')\n",
    "        break\n",
    "        \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e41c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Plot the point history\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "def plot_history(reward_history, rolling_window=20, lower_limit=None,\n",
    "                 upper_limit=None, plot_rw=True, plot_rm=True):\n",
    "    \n",
    "    if lower_limit is None or upper_limit is None:\n",
    "        rh = reward_history\n",
    "        xs = [x for x in range(len(reward_history))]\n",
    "    else:\n",
    "        rh = reward_history[lower_limit:upper_limit]\n",
    "        xs = [x for x in range(lower_limit,upper_limit)]\n",
    "    \n",
    "    df = pd.DataFrame(rh)\n",
    "    rollingMean = df.rolling(rolling_window).mean()\n",
    "\n",
    "    plt.figure(figsize=(10,7), facecolor='white')\n",
    "    \n",
    "    if plot_rw:\n",
    "        plt.plot(xs, rh, linewidth=1, color='cyan')\n",
    "    if plot_rm:\n",
    "        plt.plot(xs, rollingMean, linewidth=2, color='magenta')\n",
    "    text_color = 'black'\n",
    "        \n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('black')\n",
    "    plt.grid()\n",
    "#     plt.title(\"Total Point History\", color=text_color, fontsize=40)\n",
    "    plt.xlabel('Episode', color=text_color, fontsize=30)\n",
    "    plt.ylabel('Total Points', color=text_color, fontsize=30)\n",
    "    yNumFmt = mticker.StrMethodFormatter('{x:,}')\n",
    "    ax.yaxis.set_major_formatter(yNumFmt)\n",
    "    ax.tick_params(axis='x', colors=text_color)\n",
    "    ax.tick_params(axis='y', colors=text_color)\n",
    "    plt.show()\n",
    "\n",
    "plot_history(total_point_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a2783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev\n",
    "nbdev.export.nb_export('lunarlander.ipynb','lunarlander.py')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
